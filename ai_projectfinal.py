# -*- coding: utf-8 -*-
"""AI_ProjectFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DzQaz-l2-9NFZO_uUBYTG0PNXgDAv_nk

# Importing the Necessary Libraries
"""

import numpy as np
import pandas as pd
from sklearn import datasets
from collections import Counter
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import time
from sklearn.preprocessing import StandardScaler

"""# Lets try our hands on a small Dataset"""

##loading the datset
df = datasets.load_iris()
df = pd.DataFrame(data=df.data, columns=df.feature_names)
print(df.head())
print("The column names are: ")
print( df.columns)
print(df.shape)

irisD=datasets.load_iris()
print(irisD.data)
print(irisD.target)

"""# KNN Implementation"""

class KNN:
    def __init__(self, k=3):
        self.k = k
    ##k value can range betwen 2 and 5.
    def fitData(self, X, y):
        self.X_train = X
        self.y_train = y

    def predict(self, X):
        return [self._predict(x) for x in X]

    def _predict(self, x):
      ##1. compute the distance(euclidean)
        distances = self._euclideanDistance(x)
        ##2. Now we need the closest k
        k_indices = np.argsort(distances)[:self.k]##k is defined as 3 above
        k_nearest_labels = self.y_train[k_indices]
        ##3. now for a given point we will do majority voting as to which class it belongs to.
        most_common = Counter(k_nearest_labels).most_common(1)
        return most_common[0][0]

    def _euclideanDistance(self, x):
        return np.sqrt(np.sum((self.X_train - x) ** 2, axis=1))
'''
Here, we need have 2 points and we need to calculate the distance between them.
Euclidean Distance, Manhattan Distance etc.
We will go with euclidean distance
'''

##lets run KNN on iris dataset
X,y = irisD.data, irisD.target
print(X[0])
#print(y) ##there are 3 classes/neighbours for a new point(0,1 and 2) to which it can belong to.
print(np.unique(y))
print(np.bincount(y)) ##count of each class/group

X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.2,random_state=666)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

##let us for once visualize these data points
cmap=ListedColormap(['#FF0000','#00FF00','#0000FF'])
plt.figure()
plt.scatter(X[:,2], X[:,3], c=y, cmap=cmap, edgecolors='k',s=20)

##lets train the data on KNN
model = KNN(k=5)
model.fitData(X_train, y_train)
predictions=model.predict(X_test)
print(predictions)

##lets get the accuracies
accuracy = (np.sum(predictions==y_test)/len(y_test))*100
print(accuracy)

print("The columns are",df.columns)
print("their corresponding values are ",irisD.data[69])
print("the target/label is ", irisD.target[69])

"""The above code was taken as a reference from: https://youtu.be/rTEtEy5o3X0

# Defining Functions for loading data and evaluating features.
"""

def load_data(file_path):
    return np.loadtxt(file_path)

def evaluate_features(feature_subset, dataset, knn):
    X = dataset[:, feature_subset]
    y = dataset[:, 0]

    knn.fitData(X, y)

    start_time = time.time()
    predictions = knn.predict(X)
    accuracy = np.mean(predictions == y)
    elapsed_time = time.time() - start_time

    return accuracy, elapsed_time

"""# Forward Feature Search"""

##Now we will start from feature 1 and keep on adding more feature to it.
'''
we will go something like this
1 feature -> 2 features -> 3 features -> 4 features
This is usually done to get the best features from a dataset,
as the dataset might have some redundant features as well.
Solution? PCA, maybe.
It helps in dimensionality reduction but may not yield better features.
Any other solution ? To the Rescue: Forward selection
what is it?
For every feature see the corresponding score it can get.
Now we will keep on building our features and select those features that will
give the maximum score.
Example: Select f1 then take the next f2, then next and find the features
that give the optimal result.
..THIS IS FORWARD SEARCH..
'''
def forward_feature_search(n, dataset, early_stop_threshold=0.01):
    current_set = []
    best_acc = 0
    best_set = []
    accuracies = []

    knn = KNN()

    print(f'The number of features in this dataset are: {n}')
    print('---------------------------------------------------------------')
    startTime=time.time()
    for i in range(n):
        max_accuracy = -np.inf
        feature_to_add = None
        print(f'Level {i+1} of the search tree')
        for j in range(1, n+1):
            if j not in current_set:
                accuracy, elapsed_time = evaluate_features(current_set + [j], dataset, knn)
                print(f'Consider adding the feature {j} accuracy is {accuracy}')
                if accuracy > max_accuracy:
                    max_accuracy = accuracy
                    feature_to_add = j
        current_set.append(feature_to_add)
        print(f'On level {i+1} Added feature: {feature_to_add} to the set')
        print('---------------------------------------------------------------')
        print(f'Current set of features: {current_set} with accuracy {max_accuracy}')
        accuracies.append(max_accuracy)  # store accuracy

        if max_accuracy - best_acc < early_stop_threshold and i > 0:  # early stopping check
            print(f'Improvement of {max_accuracy - best_acc} is less than {early_stop_threshold}, stopping early.')
            break

        best_acc = max_accuracy
        best_set = current_set.copy()
    endTime=time.time()
    WhatTime=endTime-startTime
    print(f'The time taken for this forward search is: {WhatTime} seconds')

    ##plotting the graphs
    plt.plot(range(1, len(accuracies)+1), accuracies, marker='o')
    plt.xlabel('Number of Features')
    plt.ylabel('Accuracy')
    plt.title('Accuracy for Forward Feature Selection')
    plt.show()

    return best_set, best_acc

"""# Backward Elimination"""

##Now let us define the backward feature selection technique
'''
Generally we use this if we have less features.
f1-f49 then f1-f48 and so on....
Here we take all features remove >=1 featrure and see the score.
For Example for the iris dataset we can evaluate it on:
1. Score = f1+f2+f3+f4
2. score = f1+f2+f3
3. score = f1+f2+f4
4. score = f1+f3+f4
5. score = f2+f3+f4
..
.. and so on
'''
def backward_feature_search(n, dataset):
    current_set = set(range(1, n+1))
    best_acc = 0
    best_set = current_set.copy()
    elapsed_times = []  # To store elapsed times

    knn = KNN()
    startTime=time.time()
    print(f'The number of features in this dataset are: {n}')
    print('---------------------------------------------------------------')
    print(f'First set of features: {current_set} with accuracy {best_acc}')

    for i in range(n):
        max_accuracy = -np.inf
        feature_to_remove = None
        print(f'Level {i+1} of the search tree')
        for j in range(1, n+1):
            if j in current_set:
                accuracy, elapsed_time = evaluate_features(list(current_set - {j}), dataset, knn)
                print(f'Consider deleting the feature {j} accuracy is {accuracy}')
                if accuracy > max_accuracy:
                    max_accuracy = accuracy
                    feature_to_remove = j
        current_set.remove(feature_to_remove)
        elapsed_times.append(elapsed_time)  # Append elapsed time after each feature is removed
        print(f'On level {i+1} deleted feature: {feature_to_remove} from the set')
        print('---------------------------------------------------------------')
        print(f'Current set of features: {current_set} with accuracy {max_accuracy}')

        if max_accuracy > best_acc:
            best_acc = max_accuracy
            best_set = current_set.copy()
    endTime=time.time()
    WhatTime=endTime-startTime
    print(f'The time taken for this backward search is: {WhatTime} seconds')

    # Plotting
    plt.plot(range(1, n+1), elapsed_times, marker='o')
    plt.xlabel('Number of Features')
    plt.ylabel('Elapsed Time (seconds)')
    plt.title('Elapsed Time for Feature Selection')
    plt.show()

    return list(best_set), best_acc

"""# Lets Do it"""

def select_dataset(dataset_choice):
    filenames = {
        1: '/content/drive/MyDrive/AI_Datset/CS170_small_Data__1.txt',
        2: '/content/drive/MyDrive/AI_Datset/CS170_large_Data__1.txt',
        3: '/content/drive/MyDrive/AI_Datset/CS170_XXXlarge_Data__2.txt',
    }
    return load_data(filenames.get(dataset_choice, ''))

while True:
    print('---------------------------------------------------------------')
    print('Dataset Choices:')
    print('1. Small dataset')
    print('2. Large dataset')
    print('3. XXX Large dataset')
    print('0. Exit')
    dataset_choice = int(input('Enter the dataset choice: '))

    if dataset_choice == 0:
        break

    print('1. Forward Search')
    print('2. Backward Search')
    print('0. Exit')
    search_choice = int(input('Enter your choice: '))

    if search_choice == 0:
        break

    dataset = select_dataset(dataset_choice)
    n = dataset.shape[1] - 1

    if dataset is None:
        continue

    if search_choice == 1:
        best_features, best_acc = forward_feature_search(n, dataset)
    elif search_choice == 2:
        best_features, best_acc = backward_feature_search(n, dataset)
    else:
        print('Invalid choice. Please try again.')

    print('Best set is', best_features, 'with accuracy', best_acc)

"""# Lets TRY on Real World Data"""

data=pd.read_csv('/content/drive/MyDrive/AI_Datset/breast-cancer-wisconsin.csv')
data=data.drop(columns='id')
print(data.head())
print(data.shape)

'''
Here, we have 10 features and 1 target variable which is class 1 and 2;malignant or benign
'''

# Replace '?' with NaN
data.replace('?', np.nan, inplace=True)


data = data.astype(float)
data.fillna(data.median(), inplace=True)

# Separate features and target
X = data.iloc[:, :-1]
y = data.iloc[:, -1]

scaler = StandardScaler()
X = scaler.fit_transform(X)

dataset = np.column_stack((X, y))

n = dataset.shape[1] - 1
while True:
  print('****************************************************************')
  print('HERE WE ARE USING BREAST CANCER DATASET AS OUR REAL WORLD DATA')
  print('Please Select from the 2 choices provided below')
  print('1. Forward Search')
  print('2. Backward Search')
  print('0. Exit')
  search_choice = int(input('Enter your choice: '))

  if search_choice == 0:
    break

  if search_choice == 1:
    best_features, best_acc = forward_feature_search(n, dataset)
  elif search_choice == 2:
    best_features, best_acc = backward_feature_search(n, dataset)
  else:
    print('Invalid choice. Please try again.')

  print('Best set is', best_features, 'with accuracy', best_acc)

"""# Mounting Google Drive"""

from google.colab import drive
drive.mount('/content/drive')